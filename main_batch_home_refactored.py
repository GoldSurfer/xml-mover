import os
import xmltodict
import json
import re
import traceback
from datetime import datetime, timedelta
from multiprocessing import Pool, cpu_count
from tqdm import tqdm
import argparse
import time
from collections import defaultdict
import logging


#############################
# 1ï¸âƒ£  ê³µí†µ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜   #
#############################

def read_xml_with_encoding(file_path: str) -> str | None:  # ë‹¤ì–‘í•œ ì¸ì½”ë”© ì‹œë„
    """UTFâ€‘8 â†’ EUCâ€‘KR ìˆœìœ¼ë¡œ ì‹œë„í•´ì„œ XML í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜."""
    for enc in ("utf-8", "euc-kr", "cp949", "latin1"):
        try:
            with open(file_path, "r", encoding=enc) as f:
                return f.read()
        except UnicodeDecodeError:
            continue
        except FileNotFoundError:
            return None
    return None

# ğŸ“Œ xmlns="â€¦"   í˜¹ì€   xmlns:xx="â€¦"  ì „ì²´ ì œê±° (xml ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì„ ì–¸ì„ ì œê±°í•˜ì—¬ íŒŒì‹±ì„ ë‹¨ìˆœí™”)
def strip_xmlns(xml: str) -> str:
    return re.sub(r"\sxmlns(:\w+)?=\"[^\"]+\"", "", xml)

# ğŸ“Œ íƒœê·¸ ì´ë¦„ì— ë¶™ì€ prefix("abc:") ì œê±°  â†’  dict íŒŒì‹± í›„ ì¬ê·€ì ìœ¼ë¡œ key ì •ê·œí™”
def strip_prefix(obj):
    if isinstance(obj, dict):
        return {k.split(":")[-1]: strip_prefix(v) for k, v in obj.items()}
    if isinstance(obj, list):
        return [strip_prefix(i) for i in obj]
    return obj

# ğŸ“Œ ì•ˆì „í•˜ê²Œ dictionary ê°’ ê°€ì ¸ì˜¤ê¸°
def safe_dict_get(d, key, default=None):
    """ì‚¬ì „ì—ì„œ ì•ˆì „í•˜ê²Œ ê°’ì„ ê°€ì ¸ì˜¤ëŠ” í—¬í¼ í•¨ìˆ˜"""
    if not isinstance(d, dict):
        return default
    return d.get(key, default)

# ğŸ“Œ dict/list ì—ì„œ ì•ˆì „í•˜ê²Œ key ì¶”ì¶œ
def safe_get(obj, key):
    """dict / list ê¹Šì´ë¥¼ ê°€ë¦¬ì§€ ì•Šê³  key ê°’ì„ íƒìƒ‰"""
    if obj is None:
        return None
    
    # ë‹¨ìˆœ dict ë¹ ë¥¸ ì²˜ë¦¬
    if isinstance(obj, dict) and key in obj:
        return obj[key]
    
    # íƒìƒ‰ í•„ìš” ì‹œ ì‹œì‘
    queue = [obj]
    visited = set()
    results = []
    
    while queue:
        current = queue.pop(0)
        
        # ë°©ë¬¸ í™•ì¸ (ìˆœí™˜ ì°¸ì¡° ë°©ì§€)
        if isinstance(current, (dict, list)):
            current_id = id(current)
            if current_id in visited:
                continue
            visited.add(current_id)
        
        if isinstance(current, dict):
            # í˜„ì¬ dictì—ì„œ key ì°¾ê¸°
            if key in current:
                value = current[key]
                if isinstance(value, dict):
                    # text ë‚´ìš© ìš°ì„  ì¶”ì¶œ
                    if "#text" in value:
                        results.append(value["#text"])
                    elif "text" in value:
                        results.append(value["text"])
                    else:
                        results.append(value)
                elif isinstance(value, list) and value:
                    # ë¦¬ìŠ¤íŠ¸ì˜ ê²½ìš° ëª¨ë‘ ì¶”ê°€
                    for v in value:
                        queue.append(v)
                else:
                    results.append(value)
            
            # ëª¨ë“  ê°’ì„ íì— ì¶”ê°€
            for v in current.values():
                if v is not None:
                    queue.append(v)
        
        elif isinstance(current, list):
            # ë¦¬ìŠ¤íŠ¸ì˜ ëª¨ë“  ìš”ì†Œ íì— ì¶”ê°€
            for v in current:
                if v is not None:
                    queue.append(v)
    
    # ê²°ê³¼ ë°˜í™˜
    if len(results) == 1:
        return results[0]
    elif results:
        return results
    return None




# ğŸ“Œ ëª¨ë“  ë¬¸ìì—´ ë…¸ë“œë¥¼ ì•ˆì „í•˜ê²Œ ë¬¸ìì—´ë¡œ ì¶”ì¶œ
def extract_text(node):
    """ë‹¤ì–‘í•œ ë…¸ë“œì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì—¬ ë¬¸ìì—´ë¡œ ë°˜í™˜"""
    if node is None:
        return None
    
    # ë¬¸ìì—´ì´ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
    if isinstance(node, str):
        return node.strip()
    
    # ë”•ì…”ë„ˆë¦¬ ì²˜ë¦¬
    if isinstance(node, dict):
        # ì†ì„± í‚¤ëŠ” ê±´ë„ˆëœ€ (ë²ˆí˜¸ ë“±)
        clean_dict = {k: v for k, v in node.items() if not (isinstance(k, str) and k.startswith('@'))}
        
        # #text ë˜ëŠ” text íƒœê·¸ ìš°ì„  ì²˜ë¦¬
        for key in ["#text", "text"]:
            if key in node and node[key]:
                return extract_text(node[key])
        
        # ë‹¤ë¥¸ ëª¨ë“  ê°’ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        texts = []
        for v in clean_dict.values():
            text = extract_text(v)
            if text:
                texts.append(text)
        
        # ê²°í•©í•˜ì—¬ ë°˜í™˜
        if texts:
            return "\n".join(texts)
        return None
    
    # ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬
    if isinstance(node, list):
        texts = []
        for item in node:
            text = extract_text(item)
            if text:
                texts.append(text)
        return "\n".join(texts) if texts else None
    
    # ê¸°íƒ€ íƒ€ì…ì€ ë¬¸ìì—´ë¡œ ë³€í™˜
    return str(node).strip()

############################################
# 2ï¸âƒ£  íƒ€ì…ë³„ ì„¸ë¶€ íŒŒì‹± ë¡œì§ (CN, BUSINESS) #
############################################

def get_abstract_text(abstract):
    """ì¶”ìƒ/ì´ˆë¡ í…ìŠ¤íŠ¸ ì¶”ì¶œ í•¨ìˆ˜"""
    if abstract is None:
        return None
    
    # Paragraphs íƒœê·¸ê°€ ìˆëŠ” ê²½ìš° ì²˜ë¦¬
    paragraphs = None
    if isinstance(abstract, dict):
        paragraphs = abstract.get("Paragraphs") or abstract.get("p")
    
    if paragraphs:
        return extract_text(paragraphs)
    
    # ì§ì ‘ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    return extract_text(abstract)

def extract_claims(root):
    """XMLì—ì„œ ì²­êµ¬í•­ ì¶”ì¶œ"""
    # 1. Business(êµ¬ë²„ì „) í¬ë§· ì²­êµ¬í•­ (Claims/Claim) -> ì •í™•íˆëŠ” Claims/Claim/ClaimText ì„!
    claims_section = None
    if isinstance(root, dict):
        if "Claims" in root:
            claims_section = root["Claims"]
    
    if claims_section:
        claim_list = safe_get(claims_section, "Claim")
        if claim_list:
            all_claims = []
            
            # ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ì²˜ë¦¬
            if isinstance(claim_list, list):
                for i, claim in enumerate(claim_list, 1):
                    if not isinstance(claim, dict):
                        continue
                        
                    claim_num = safe_dict_get(claim, "@num", str(i))
                    if "ClaimText" in claim:
                        claim_text = extract_text(claim["ClaimText"])
                    else:
                        claim_text = extract_text(claim)
                        
                    if claim_text:
                        all_claims.append(f"{claim_num}. {claim_text}")
            # ë‹¨ì¼ ì²­êµ¬í•­
            elif isinstance(claim_list, dict):
                claim_num = safe_dict_get(claim_list, "@num", "1")
                if "ClaimText" in claim_list:
                    claim_text = extract_text(claim_list["ClaimText"])
                else:
                    claim_text = extract_text(claim_list)
                    
                if claim_text:
                    all_claims.append(f"{claim_num}. {claim_text}")
            
            if all_claims:
                return "Â¶".join(all_claims)
    
    # # 2. Business ì†ì„± ê¸°ë°˜ ì²­êµ¬í•­ (ë‹¨ìˆœ í…ìŠ¤íŠ¸) -> [ìˆ˜ì • ê³ ë ¤ì‚¬í•­] í˜„ì‹¤ì ìœ¼ë¡œ ìˆê¸° í˜ë“  xml êµ¬ì¡°
    # if isinstance(root, dict) and "Claims" in root and isinstance(root["Claims"], str):
    #     return root["Claims"]

    # # 3. Business ì²­êµ¬í•­ ì§ì ‘ ì ‘ê·¼ ë°©ì‹ -> [ìˆ˜ì • ê³ ë ¤ì‚¬í•­] 1ë²ˆì—ì„œ ì´ë¯¸ ì»¤ë²„í•¨
    # if isinstance(root, dict) and "Claims" in root and not isinstance(root["Claims"], str):
    #     return extract_text(root["Claims"])
        
    # 4. CN í¬ë§· ì²­êµ¬í•­
    if isinstance(root, dict) and "application-body" in root and isinstance(root["application-body"], dict):
        claims_section = safe_dict_get(root["application-body"], "claims")
        if claims_section and isinstance(claims_section, dict):
            claim_list = safe_get(claims_section, "claim")
            if claim_list:
                all_claims = []
                
                # ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ì²˜ë¦¬
                if isinstance(claim_list, list):
                    for i, claim in enumerate(claim_list, 1):
                        if not isinstance(claim, dict):
                            continue
                            
                        claim_num = safe_dict_get(claim, "@num", str(i))
                        if "claim-text" in claim:
                            claim_text = extract_text(claim["claim-text"])
                        else:
                            claim_text = extract_text(claim)
                            
                        if claim_text:
                            all_claims.append(f"{claim_text}")

                # ë‹¨ì¼ ì²­êµ¬í•­
                elif isinstance(claim_list, dict):
                    claim_num = safe_dict_get(claim_list, "@num", "1")
                    if "claim-text" in claim_list:
                        claim_text = extract_text(claim_list["claim-text"])
                    else:
                        claim_text = extract_text(claim_list)
                        
                    if claim_text:
                        all_claims.append(f"{claim_text}")
                
                if all_claims:
                    return "Â¶".join(all_claims)
    
    # # 5. KR í¬ë§· ì²­êµ¬í•­ -> í˜„ì¬ ì¤‘êµ­ íŠ¹í—ˆì— ëŒ€í•œ ì‘ì—… ì¤‘!
    # if isinstance(root, dict) and "claims" in root:
    #     return extract_text(root["claims"])
    
    # 6. ë”¥ ì„œì¹˜
    deep_claims = safe_get(root, "claim-text") or safe_get(root, "ClaimText")
    if deep_claims:
        return extract_text(deep_claims)
    
    return None

# ğŸ“Œ IPC í…ìŠ¤íŠ¸ì—ì„œ ê´„í˜¸ì™€ ë‚ ì§œ ì œê±°í•˜ëŠ” í•¨ìˆ˜ ì¶”ê°€
def clean_ipc_text(ipc_text):
    """IPC í…ìŠ¤íŠ¸ ì •ë¦¬: ê³µë°± ì •ë¦¬ ë° ê´„í˜¸(ë‚ ì§œ í¬í•¨) ì œê±°"""
    if not ipc_text:
        return None
    # ê´„í˜¸ì™€ ê·¸ ì•ˆì˜ ë‚´ìš© ì œê±°
    cleaned_text = re.sub(r'\([^)]*\)', '', str(ipc_text))
    # ê³µë°± ì •ë¦¬ (ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ)
    cleaned_text = ' '.join(cleaned_text.split())
    return cleaned_text.strip()

# ğŸ“Œ ê¸°ê´€ ì´ë¦„ì—ì„œ ê´„í˜¸ì™€ ìˆ«ì ì œê±°í•˜ëŠ” í•¨ìˆ˜ ì¶”ê°€
def clean_organization_name(org_name):
    """ê¸°ê´€ ì´ë¦„ì—ì„œ ê´„í˜¸ì™€ ìˆ«ì ì œê±°"""
    if not org_name:
        return None
    # ê´„í˜¸ì™€ ê·¸ ì•ˆì˜ ë‚´ìš© ì œê±°
    cleaned_name = re.sub(r'\([^)]*\)', '', str(org_name))
    # ë’¤ì— ë¶™ì€ ìˆ«ì ì œê±°
    cleaned_name = re.sub(r'\s*\d+$', '', cleaned_name)
    return cleaned_name.strip()

# ğŸ“Œ êµ¬í˜• ë¬¸ì„œì˜ "ë„ë©´ì˜ ê°„ë‹¨í•œ ì„¤ëª…"ê³¼ "ë°œëª…ì„ ì‹¤ì‹œí•˜ê¸° ìœ„í•œ êµ¬ì²´ì ì¸ ë‚´ìš©" êµ¬ë¶„ í•¨ìˆ˜ ì¶”ê°€
def extract_description_sections(paragraphs):
    """ë‹¨ë½ ëª©ë¡ì—ì„œ ë„ë©´ ì„¤ëª…ê³¼ ì‹¤ì‹œì˜ˆ ì„¹ì…˜ë§Œ ì¶”ì¶œí•˜ê³  ì „ì²´ ì„¤ëª…ë„ í•¨ê»˜ ë°˜í™˜"""
    # ë„ë©´ ì„¤ëª… ë§ˆì»¤ (í™•ì¥ì„± ìœ„í•´ í‚¤ì›Œë“œ ì¶”ê°€)
    DRAWING_MARKERS = [
        "[ë„ë©´ì˜ ê°„ë‹¨í•œ ì„¤ëª…]", "ë„ë©´ì˜ ê°„ë‹¨í•œ ì„¤ëª…"
    ]
    
    # ì‹¤ì‹œì˜ˆ ë§ˆì»¤ (í™•ì¥ì„± ìœ„í•´ í‚¤ì›Œë“œ ì¶”ê°€)
    EMBODIMENT_MARKERS = [
        "[ë°œëª…ì„ ì‹¤ì‹œí•˜ê¸° ìœ„í•œ êµ¬ì²´ì ì¸ ë‚´ìš©]", "ë”ìš± ìƒì„¸í•˜ê²Œ ì„¤ëª…í•œë‹¤", "êµ¬ì²´ì ì¸ ì‹¤ì‹œë°©ì‹:"
    ]
    
    # ê²°ê³¼ ì´ˆê¸°í™”
    brief_description_of_drawings = None
    description_of_embodiments = None
    
    # ì „ì²´ description ë‚´ìš© ì¶”ì¶œ (ëª¨ë“  í•„í„°ë§ ì œê±°)
    full_description = []
    for p in paragraphs:
        text = extract_text(p)
        if text:  # ëª¨ë“  í…ìŠ¤íŠ¸ í¬í•¨, í•„í„°ë§ ì—†ìŒ
            full_description.append(text)
    
    # ì „ì²´ ë‚´ìš© í•©ì¹˜ê¸°
    full_description_text = "\n\n".join(full_description) if full_description else None
    
    # ì¸ë±ìŠ¤ ì´ˆê¸°í™”
    drawings_start_idx = -1
    embodiment_start_idx = -1
    
    # 1. ë§ˆì»¤ë¡œ ì„¹ì…˜ ì‹œì‘ì  ì°¾ê¸°
    for i, p in enumerate(paragraphs):
        text = extract_text(p)
        if not text:
            continue
            
        # í…ìŠ¤íŠ¸ ì •ê·œí™”
        lower_text = text.lower()
            
        # ë„ë©´ ì„¤ëª… ì‹œì‘ì  í™•ì¸ - ê¸¸ì´ ì¡°ê±´ ì™„í™”
        if any(marker.lower() in lower_text for marker in DRAWING_MARKERS) and len(text) < 100:
            drawings_start_idx = i
            continue
            
        # ì‹¤ì‹œì˜ˆ ì‹œì‘ì  í™•ì¸ - ê¸¸ì´ ì¡°ê±´ ì™„í™”
        if any(marker.lower() in lower_text for marker in EMBODIMENT_MARKERS) and len(text) < 100:
            # ë„ë©´ ì„¤ëª…ì˜ ëì ë„ ì—¬ê¸°ë¡œ ì •ì˜ ê°€ëŠ¥
            if drawings_start_idx != -1 and drawings_start_idx < i:
                # ë„ë©´ ì„¤ëª… ëì ì´ ì‹¤ì‹œì˜ˆ ì‹œì‘ì ìœ¼ë¡œ ê°„ì£¼
                drawing_paragraphs = []
                for j in range(drawings_start_idx + 1, i):
                    drawing_text = extract_text(paragraphs[j])
                    if drawing_text: # and not is_table_like(drawing_text): 'í…Œì´ë¸” í•„í„°ë§' ì œê±°
                        drawing_paragraphs.append(drawing_text)
                
                if drawing_paragraphs:
                    brief_description_of_drawings = "\n\n".join(drawing_paragraphs)
            
            embodiment_start_idx = i
            continue
    
    # 2. ì‹¤ì‹œì˜ˆ ì„¹ì…˜ì€ description íƒœê·¸ê°€ ëë‚˜ëŠ” ë¶€ë¶„ê¹Œì§€ (ëª¨ë“  ë‚¨ì€ ë‹¨ë½)
    if embodiment_start_idx != -1:
        embodiment_paragraphs = []
        for j in range(embodiment_start_idx + 1, len(paragraphs)):
            embodiment_text = extract_text(paragraphs[j])
            if embodiment_text: # and not is_table_like(embodiment_text): 'í…Œì´ë¸” í•„í„°ë§' ì œê±°
                embodiment_paragraphs.append(embodiment_text)
        
        if embodiment_paragraphs:
            description_of_embodiments = "\n\n".join(embodiment_paragraphs)
    
    # 3. ë„ë©´ ì„¤ëª… ë§ˆì»¤ë§Œ ìˆê³  ì‹¤ì‹œì˜ˆ ë§ˆì»¤ê°€ ì—†ëŠ” ê²½ìš°
    # ë„ë©´ ì„¤ëª… ë¶€ë¶„ì„ ë³„ë„ë¡œ ì¶”ì¶œí•˜ì§€ ì•Šê³ , description ì „ì²´ë¥¼ ë°˜í™˜
    if drawings_start_idx != -1 and embodiment_start_idx == -1:
        # brief_description_of_drawingsëŠ” Noneìœ¼ë¡œ ìœ ì§€
        # ë„ë©´ ì„¤ëª…ì€ ì „ì²´ descriptionì— í¬í•¨ë˜ì–´ ìˆìŒ
        # ë„ë©´ ì„¤ëª… êµ¬ë¶„ ì—†ì´ description ì „ì²´ë¥¼ ì°¸ì¡°í•˜ë„ë¡ í•¨
        pass
    
    return brief_description_of_drawings, description_of_embodiments, full_description_text

def is_table_like(text):
    """í…Œì´ë¸” í˜•ì‹ì˜ í…ìŠ¤íŠ¸ì¸ì§€ í™•ì¸í•©ë‹ˆë‹¤"""
    if not text:
        return False
        
    # 1. ìˆ«ìì™€ íŠ¹ìˆ˜ë¬¸ì ë¹„ìœ¨ì´ ë†’ì€ ê²½ìš°
    numeric_ratio = sum(c.isdigit() or c in '-+*/,.()%' for c in text) / max(len(text), 1)
    if numeric_ratio > 0.4:  # ê²½í—˜ì  ì„ê³„ê°’
        return True
        
    # 2. í•œê¸€ì´ ì—†ê³  ì¤‘êµ­ì–´/ì˜ì–´ë§Œ ìˆëŠ” ê²½ìš° (ì´ ì¡°ê±´ë„ ì œê±°)
    # has_korean = any('\uAC00' <= c <= '\uD7A3' for c in text)
    # if not has_korean and any('\u4e00' <= c <= '\u9fff' for c in text):
    #     return True
        
    # # 3. í‘œ í˜•ì‹ì˜ ì‹œì‘ íŒ¨í„´ (ì˜ì–´ íŒ¨í„´ ì¶”ê°€)
    # text_start = text.strip().lower()
    # table_starts = [
    #     'åºåˆ—', 'måºåˆ—', 'preferred gold', 'katsami', 'sequence', 'm sequence', 
    #     'qs(og-', 'gold', 'snr', 'ê³„ì‚°', '4-qs', '8-qs', '16-qs', 
    #     '32-qs', '64-qs', '128-qs', '256-qs', '512-qs', '1024-qs',
    #     'table', 'í‘œ', 'í‘œ:', 'ë„í‘œ'
    # ]
    
    # if any(text_start.startswith(marker) for marker in table_starts):
    #     return True
        
    # # 4. í‘œì— ìì£¼ ì‚¬ìš©ë˜ëŠ” ì˜ì–´ ë‹¨ì–´ë¡œ ì‹œì‘í•˜ëŠ” ì¤„
    # table_marker_words = ('sequence', 'length', 'number', 'ratio', 'level', 'signal', 'method')
    # if any(text_start.startswith(word) for word in table_marker_words):
    #     return True
        
    # # 5. ì§§ì€ ì¤„ì´ ì—¬ëŸ¬ ê°œ ë°˜ë³µë˜ëŠ” íŒ¨í„´ (í…Œì´ë¸” í–‰)
    # if len(text.strip()) < 30 and text.count('\n') > 2:
    #     return True
    
    # 6. ì¤‘êµ­ì–´ ë¹„ìœ¨ ì¡°ê±´ ì œê±°
    # chinese_chars = sum(1 for c in text if '\u4e00' <= c <= '\u9fff')
    # chinese_ratio = chinese_chars / max(len(text), 1)
    # if chinese_ratio > 0.2:
    #     return True
        
    return False

def scrub_tables(raw_text):
    """í‘œì™€ ì¤‘êµ­ì–´ í…ìŠ¤íŠ¸ë¥¼ ì œê±°í•˜ê³  ê¹¨ë—í•œ ë‚´ìš©ë§Œ ë°˜í™˜"""
    if not raw_text:
        return None
        
    clean_lines = []
    for line in raw_text.splitlines():
        if not line.strip():
            continue
        if is_table_like(line):
            continue
        clean_lines.append(line)
    
    result = "\n\n".join(clean_lines).strip()
    return result if result else None

# ì¶œì›ì¼ìì™€ ê³µê°œì¼ì ì‚¬ì´ì˜ ê¸°ê°„ì„ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜ ì¶”ê°€
def calculate_date_diff(app_date, pub_date):
    """ì¶œì›ì¼ìì™€ ê³µê°œì¼ì ì‚¬ì´ì˜ ê¸°ê°„(ê°œì›”)ì„ ê³„ì‚°"""
    if not app_date or not pub_date:
        return None
    
    try:
        # ë‚ ì§œ í˜•ì‹ì€ YYYYMMDDë¡œ ê°€ì •
        app_date_obj = datetime.strptime(str(app_date), "%Y%m%d")
        pub_date_obj = datetime.strptime(str(pub_date), "%Y%m%d")
        
        # ë‚ ì§œ ì°¨ì´ ê³„ì‚° (ì›” ë‹¨ìœ„)
        diff_months = (pub_date_obj.year - app_date_obj.year) * 12 + (pub_date_obj.month - app_date_obj.month)
        return diff_months
    except (ValueError, TypeError):
        return None

def format_numbers(app_number, pub_number, pub_date, kind, app_date=None):
    """íŠ¹í—ˆ ë²ˆí˜¸ í˜•ì‹ ë³€í™˜ - ì¶œì›ì¼ìì™€ ê³µê°œì¼ì ê°„ ê¸°ê°„ì„ ê³ ë ¤í•˜ì—¬ ì²˜ë¦¬"""
    clean_app = clean_application_number(app_number)
    
    publication_number = None
    publication_date = None
    open_number = None
    open_date = None
    register_number = None
    register_date = None
    
    # months_diff = calculate_date_diff(app_date, pub_date) # Kind 'Y'ëŠ” ê³µê°œì œë„ ì—†ìœ¼ë¯€ë¡œ ì´ ê³„ì‚° ë¶ˆí•„ìš”

    if kind and kind.upper() in ['B', 'C']: # ë“±ë¡ íŠ¹í—ˆ, ë“±ë¡ ì‹¤ìš©ì‹ ì•ˆ(êµ¬ ì½”ë“œ)
        publication_number = pub_number if pub_number else None # ê³µê³ ë²ˆí˜¸
        publication_date = pub_date # ê³µê³ ì¼ì
        
        # ë“±ë¡ë²ˆí˜¸ëŠ” ì¶œì›ë²ˆí˜¸ì™€ ë™ì¼í•˜ê²Œ ì„¤ì •
        register_number = clean_app if clean_app else None
        register_date = pub_date

        # ê³µê°œ ì •ë³´ ì²˜ë¦¬ (19ê°œì›” ë£°ì€ ì¼ë°˜ íŠ¹í—ˆì— ì£¼ë¡œ í•´ë‹¹, ë“±ë¡ëœ ê²ƒì€ ê³µê³  ì •ë³´ë¥¼ ë”°ë¦„)
        # ë§Œì•½ app_dateì™€ pub_dateê°€ ìˆê³ , ê·¸ ì°¨ì´ê°€ 19ê°œì›” ì´ìƒì´ë©´ ê³µê°œë˜ì—ˆì„ ìˆ˜ ìˆìŒ.
        # í•˜ì§€ë§Œ ë“±ë¡ëœ ê²½ìš°, ê³µê³  ì •ë³´ê°€ ìš°ì„ ì‹œ ë¨. ì—¬ê¸°ì„œëŠ” ì¼ë‹¨ nullë¡œ ë‘ê±°ë‚˜,
        # ë” ëª…í™•í•œ ê·œì¹™ì´ ìˆë‹¤ë©´ í•´ë‹¹ ê·œì¹™ì„ ë”°ë¼ì•¼ í•¨.
        # í˜„ì¬ëŠ” B, Cì˜ ê²½ìš° open_number/dateë¥¼ ë³„ë„ë¡œ ì„¤ì •í•˜ì§€ ì•Šê³  ìˆìŒ (ê¸°ì¡´ ë¡œì§ ìœ ì§€).
        # í•„ìš”ì‹œ calculate_date_diff ë° ê´€ë ¨ ë¡œì§ ì—¬ê¸°ì— ì ìš© ê°€ëŠ¥.
        months_diff_for_BC = calculate_date_diff(app_date, pub_date)
        if months_diff_for_BC is not None:
            if months_diff_for_BC > 18: # 19ê°œì›” ì´ˆê³¼ ì‹œ ê³µê°œëœ ê²ƒìœ¼ë¡œ ê°„ì£¼í•˜ë‚˜, ì •ë³´ ì—†ìœ¼ë©´ ê³µê³ ë²ˆí˜¸ ì‚¬ìš©
                open_number = pub_number 
                open_date = None # ê³µê°œì¼ìëŠ” íŠ¹ì • ë¶ˆê°€
            # else: 19ê°œì›” ë¯¸ë§Œì´ë©´ ê³µê°œ ì—†ì´ ë°”ë¡œ ë“±ë¡, open_number/dateëŠ” None (ê¸°ë³¸ê°’)
        # else: ë‚ ì§œ ì •ë³´ ë¶€ì¡± ì‹œ open_number/dateëŠ” None (ê¸°ë³¸ê°’)


    elif kind and kind.upper() in ['U', 'Y']: # ì‹¤ìš©ì‹ ì•ˆ (U: êµ¬, Y: ì‹ )
        # ì‹¤ìš©ì‹ ì•ˆì€ ê³µê°œ ì œë„ê°€ ì—†ìœ¼ë¯€ë¡œ OpenNumber, OpenDateëŠ” í•­ìƒ null
        open_number = None
        open_date = None
        
        # PublicationNumber/DateëŠ” ê³µê³ ë²ˆí˜¸/ì¼ìë¡œ ì„¤ì • (ë“±ë¡ ê°„ì£¼)
        publication_number = pub_number if pub_number else None
        publication_date = pub_date
        
        # RegisterNumberëŠ” ì¶œì›ë²ˆí˜¸ì™€ ë™ì¼í•˜ê²Œ ì„¤ì •
        register_number = clean_app if clean_app else None
        register_date = pub_date
        
    elif kind and kind.upper() == 'A': # ê³µê°œ íŠ¹í—ˆ
        open_number = pub_number if pub_number else None
        open_date = pub_date
        # PublicationNumber, PublicationDate ë“±ì€ null (ì•„ì§ ê³µê³ /ë“±ë¡ ì „)
        # register_number, register_dateë„ null

    # ìµœì¢… ë°˜í™˜ ì‹œ ì¶œì›ë²ˆí˜¸ëŠ” clean_app ì‚¬ìš©
    return clean_app, publication_number, publication_date, open_number, open_date, register_number, register_date

def parse_cn_patent(root: dict) -> dict:
    """<cn-patent-document> ì „ìš© íŒŒì„œ"""
    biblio = safe_dict_get(root, "cn-bibliographic-data", {})
    parties = safe_dict_get(biblio, "cn-parties", {})

    # ê¸°ë³¸ ë¬¸ì„œ ì •ë³´ ì¶”ì¶œ
    pub_ref_container = safe_dict_get(biblio, "cn-publication-reference", {})
    pub_ref = safe_get(pub_ref_container, "document-id") or {}
    app_ref_container = safe_dict_get(biblio, "application-reference", {})
    app_ref = safe_get(app_ref_container, "document-id") or {}

    # ë”•ì…”ë„ˆë¦¬ íƒ€ì… ê²€ì¦
    if not isinstance(pub_ref, dict): pub_ref = {}
    if not isinstance(app_ref, dict): app_ref = {}
        
    # ë²ˆí˜¸ì™€ ë‚ ì§œ ì¶”ì¶œ
    doc_number = safe_dict_get(pub_ref, "doc-number")
    app_number = safe_dict_get(app_ref, "doc-number")
    pub_date = safe_dict_get(pub_ref, "date")
    app_date = safe_dict_get(app_ref, "date")
    kind = safe_dict_get(pub_ref, "kind")
    
    # ë²ˆí˜¸ í˜•ì‹ ë³€í™˜
    app_number_clean, pub_number_formatted, pub_date_formatted, open_number, open_date, register_number, register_date = format_numbers(app_number, doc_number, pub_date, kind, app_date)
        
    # CPC/IPC ì½”ë“œ
    cpc_text = safe_get(safe_get(safe_dict_get(biblio, "classifications-ipcr", {}), "classification-ipcr"), "text")
    main_cpc = None
    if isinstance(cpc_text, str):
        main_cpc = clean_ipc_text(cpc_text)
    elif isinstance(cpc_text, list) and cpc_text:
        main_cpc = clean_ipc_text(cpc_text[0])

    # ì¶œì›ì¸ ì •ë³´
    applicants = safe_get(safe_get(safe_dict_get(parties, "cn-applicants", {}), "cn-applicant"), "name")
    applicant_name = None
    if isinstance(applicants, list) and applicants:
        applicant_name = clean_organization_name(applicants[0])
    elif applicants:
        applicant_name = clean_organization_name(applicants)
        
    # ë°œëª…ì ì •ë³´
    inventors = safe_get(safe_get(safe_dict_get(parties, "cn-inventors", {}), "cn-inventor"), "name")
    inventor_name = None
    if isinstance(inventors, list):
        inventor_name = ", ".join([i for i in inventors if i])
    elif inventors:
        inventor_name = str(inventors)

    # ëŒ€ë¦¬ì¸ ì •ë³´
    agents_block = safe_dict_get(parties, "cn-agents", {})
    agent_entries = safe_get(agents_block, "cn-agent") if isinstance(agents_block, dict) else None
    agent_name_combined = None
    
    if agent_entries:
        if not isinstance(agent_entries, list):
            agent_entries = [agent_entries]
        agents_formatted = []
        for ag in agent_entries:
            if not isinstance(ag, dict):
                continue
            ind_name = safe_dict_get(ag, "name", "")
            agency = safe_get(safe_dict_get(ag, "cn-agency", {}), "name") or ""
            agency = clean_organization_name(agency)
            agents_formatted.append(f"{ind_name} ({agency})".strip())
        agent_name_combined = "; ".join(agents_formatted)

    # ë°œëª… ì œëª©
    title = safe_dict_get(biblio, "invention-title")
    if isinstance(title, dict):
        title = safe_dict_get(title, "#text") or safe_dict_get(title, "text")
    
    # ìš”ì•½ ì •ë³´
    abstract = safe_dict_get(biblio, "abstract")
    summary = get_abstract_text(abstract)
    
    # ì²­êµ¬í•­
    claims = extract_claims(root)
    
    # ìƒì„¸ ì„¤ëª… ì¶”ì¶œ
    description = find_description_in_document(root)
    all_paragraphs = extract_description_paragraphs(description)
    drawing_section, embodiment_section, full_description_text = extract_structured_description(description, all_paragraphs)
    
    # ê²°ê³¼ ìƒì„± ë° ë°˜í™˜
    return create_parsed_result(
        main_cpc, kind, open_number, open_date, register_number, register_date,
        pub_number_formatted, pub_date_formatted, app_number_clean, app_date,
        applicant_name, inventor_name, agent_name_combined, title, summary,
        drawing_section, embodiment_section, full_description_text, claims
    )

def parse_business(root: dict) -> dict:
    """<PatentDocumentAndRelated> íƒ€ì… íŒŒì„œ"""
    biblio = safe_dict_get(root, "BibliographicData", {})
    
    # --- ê³µê°œÂ·ì¶œì› ì •ë³´ ì¶”ì¶œ ---
    pub_info, pub_doc = extract_publication_info(root, biblio)
    
    # ê³µê°œë²ˆí˜¸, ê³µê°œì¼ì ì¶”ì¶œ
    pub_no, pub_dt = safe_dict_get(pub_doc, "DocNumber"), safe_dict_get(pub_doc, "Date")
    
    # ê³µê°œë²ˆí˜¸ê°€ ì—†ì„ ê²½ìš° ë¬¸ì„œì—ì„œ ì§ì ‘ ì¶”ì¶œ ì‹œë„
    if not pub_no:
        pub_no = safe_dict_get(pub_info, "DocNumber") or safe_dict_get(root, "@docNumber")
            
    # ê³µê°œì¼ìê°€ ì—†ì„ ê²½ìš°
    if not pub_dt:
        pub_dt = safe_dict_get(pub_info, "Date")

    # ì¶œì›ë²ˆí˜¸, ì¶œì›ì¼ì ì¶”ì¶œ
    application_number, application_date = extract_application_info(biblio)
    
    # --- CPC / IPC ì¶”ì¶œ ---
    main_cpc = extract_cpc_info(root, biblio)
    
    # --- ë‹¹ì‚¬ì ì •ë³´ ì¶”ì¶œ ---
    applicant_name = extract_applicant_info(biblio)
    inventor_name = extract_inventor_info(biblio)
    agent_name = extract_agent_info(biblio)
    
    # --- ë°œëª… ì •ë³´ ì¶”ì¶œ ---
    title = extract_text(safe_get(biblio, "InventionTitle"))
    abstract = safe_get(root, "Abstract") or safe_get(biblio, "Abstract")
    summary = get_abstract_text(abstract)
    claims = extract_claims(root)
    
    # --- ìƒì„¸ ì„¤ëª… ì¶”ì¶œ ---
    description = find_description_in_document(root)
    all_paragraphs = extract_description_paragraphs(description)
    drawing_section, embodiment_section, full_description_text = extract_structured_description(description, all_paragraphs)
    
    # --- ë²ˆí˜¸ í˜•ì‹ ë³€í™˜ ---
    kind = safe_dict_get(pub_doc, "Kind") or safe_dict_get(root, "@kind")
    app_number_clean, pub_number_formatted, pub_date_formatted, open_number, open_date, register_number, register_date = format_numbers(application_number, pub_no, pub_dt, kind, application_date)
    
    # Kindê°€ B ë˜ëŠ” Cì¸ ê²½ìš° RegisterDateê°€ nullì´ë©´ ì§ì ‘ ì„¤ì •
    if kind and kind.upper() in ['B', 'C'] and not register_date and pub_dt:
        register_date = pub_dt
    
    return create_parsed_result(
        main_cpc, kind, open_number, open_date, register_number, register_date,
        pub_number_formatted, pub_date_formatted, app_number_clean, application_date,
        applicant_name, inventor_name, agent_name, title, summary,
        drawing_section, embodiment_section, full_description_text, claims
    )

# ì¶œì›ë²ˆí˜¸ì—ì„œ ì†Œìˆ˜ì  ë° ì†Œìˆ˜ì  ì´í•˜ì˜ ìˆ«ìë¥¼ ì œê±°í•˜ëŠ” í•¨ìˆ˜
def clean_application_number(app_number):
    """ì¶œì›ë²ˆí˜¸ì—ì„œ ì†Œìˆ˜ì  ë° ì†Œìˆ˜ì  ì´í•˜ì˜ ìˆ«ìë¥¼ ì œê±°"""
    if not app_number:
        return app_number
    # ë¬¸ìì—´ë¡œ ë³€í™˜
    app_number_str = str(app_number)
    # ì†Œìˆ˜ì ì´ ìˆëŠ” ê²½ìš° ì†Œìˆ˜ì  ì´í•˜ ì œê±°
    if '.' in app_number_str:
        return app_number_str.split('.')[0]
    return app_number_str

###################################
# 3ï¸âƒ£  ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬ ì§„ì… í•¨ìˆ˜  #
###################################

def process_xml_file(file_path: str): # file_pathëŠ” ì´ë¯¸ ì „ì²´ ê²½ë¡œë¥¼ ê°€ì§€ê³  ìˆìŒ
    """XML íŒŒì¼ì„ íŒŒì‹±í•˜ì—¬ JSON ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ë©°, (íŒŒì¼ê²½ë¡œ, ê²°ê³¼ë°ì´í„°, ì˜¤ë¥˜, ì¶”ì¶œìƒíƒœ, ëˆ„ë½ì •ë³´) í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    extraction_status = {}
    missing_details = None 
    try:
        xml_txt = read_xml_with_encoding(file_path)
        if xml_txt is None:
            return file_path, None, f"ì¸ì½”ë”©ì„ ì¸ì‹í•  ìˆ˜ ì—†ìŒ", extraction_status, missing_details

        xml_txt = strip_xmlns(xml_txt)
        data = xmltodict.parse(xml_txt)
        data = strip_prefix(data)

        root_key = next(iter(data))
        root = data[root_key]
        parsed = None
        
        if root_key.lower().startswith("cn-patent-document"):
            parsed = parse_cn_patent(root)
        elif root_key.lower().endswith("patentdocumentandrelated"):
            parsed = parse_business(root)
        else:
            parsed = process_fallback_xml(root)
            
        if not parsed:
            return file_path, None, "ì•Œ ìˆ˜ ì—†ëŠ” XML êµ¬ì¡°", extraction_status, missing_details

        parsed.update({
            "meta": {
                "convertedDate": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "originalFileName": os.path.basename(file_path)
            }
        })

        for key, value in parsed.items():
            if key != "meta":
                extraction_status[key] = value is not None and value != ""
        
        detected_missing_fields = [field for field, extracted in extraction_status.items() if not extracted]

        if detected_missing_fields:
            current_contextual_issues = [] # ì—¬ëŸ¬ ë¬¸ë§¥ì  ì´ìŠˆë¥¼ ë‹´ì„ ë¦¬ìŠ¤íŠ¸

            # --- ì¡°ê±´ 1: ë„ë©´ ë˜ëŠ” ì‹¤ì‹œì˜ˆ ì„¤ëª… ëˆ„ë½ ì‹œ Description ëˆ„ë½ ---
            brief_drawings_present = bool(parsed.get("BriefDescriptionOfDrawings"))
            embodiments_present = bool(parsed.get("DescriptionOfEmbodiments"))
            if not brief_drawings_present or not embodiments_present:
                if "Description" in detected_missing_fields:
                    current_contextual_issues.append("Description_missing_when_drawings_or_embodiments_absent")
            
            # --- ì¡°ê±´ 2: Kind 'A'ì¼ ë•Œ OpenNumber ë˜ëŠ” OpenDate ëˆ„ë½ ---
            doc_kind = parsed.get("Kind")
            if doc_kind == 'A':
                missing_A_fields = []
                if "OpenNumber" in detected_missing_fields:
                    missing_A_fields.append("OpenNumber")
                if "OpenDate" in detected_missing_fields:
                    missing_A_fields.append("OpenDate")
                
                if missing_A_fields: # OpenNumber ë˜ëŠ” OpenDate ì¤‘ í•˜ë‚˜ë¼ë„ ëˆ„ë½ëœ ê²½ìš°
                    current_contextual_issues.append(f"Kind_A_missing_{'_and_'.join(missing_A_fields)}")

            # missing_details ìƒì„±
            missing_details = {
                "full_file_path": file_path,
                "missing_fields": detected_missing_fields,
                "contextual_issues": current_contextual_issues if current_contextual_issues else None 
                # contextual_issues í‚¤ë¡œ ë³€ê²½í•˜ê³ , ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìœ¼ë©´ None
            }
        
        return file_path, parsed, None, extraction_status, missing_details

    except Exception as e:
        traceback_str = traceback.format_exc()
        error_msg = f"Error: {str(e)}\\nTraceback: {traceback_str}"
        # extraction_statusëŠ” ë¹ˆ dict, missing_detailsëŠ” Noneìœ¼ë¡œ ë°˜í™˜
        return file_path, None, error_msg, {}, None # ìˆ˜ì •ëœ ë°˜í™˜ê°’: (file_path, data, error, extraction_status, missing_details)

#################################
# 4ï¸âƒ£  ë©€í‹°â€‘í”„ë¡œì„¸ìŠ¤ ë°°ì¹˜ ì‹¤í–‰ #
#################################

def collect_xmls_for_year(input_folder, year):
    """ì—°ë„ë³„ XML íŒŒì¼ ìˆ˜ì§‘"""
    year_dir = os.path.join(input_folder, year)
    if not os.path.isdir(year_dir):
        print(f"   â€¢ {year} í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return []
        
    xml_files = []
    print(f"[ìŠ¤ìº”] {year} í´ë”ì—ì„œ XML ìˆ˜ì§‘ ì¤‘...")
    
    for root, _, files in os.walk(year_dir):
        for file in files:
            if file.lower().endswith('.xml'):
                xml_files.append(os.path.join(root, file))
    
    print(f"[ì™„ë£Œ] {year}: {len(xml_files)}ê°œ XML íŒŒì¼ ë°œê²¬")
    return xml_files

def generate_stats_dict(items_count, field_success_counts, kind_A_stats, desc_fallback_stats):
    """í†µê³„ ì •ë³´ë¥¼ ë‹´ì€ ë”•ì…”ë„ˆë¦¬ë¥¼ ìƒì„±í•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
    stats_output = {
        "summary": {
            "total_processed_items": items_count,
        },
        "field_extraction_success_rate": [],
        "kind_A_special_stats": {},
        "description_fallback_stats": {},
        # "missing_item_details": missing_details_list if missing_details_list is not None else [] # ì´ ë¶€ë¶„ ì œê±°
    }

    if items_count > 0:
        for field, count in sorted(field_success_counts.items()):
            rate = (count / items_count * 100)
            stats_output["field_extraction_success_rate"].append({
                "field": field,
                "success_count": count,
                "total_items": items_count,
                "success_rate_percent": round(rate, 2)
            })

    if kind_A_stats.get("total_A_items", 0) > 0:
        total_A = kind_A_stats["total_A_items"]
        open_num_success = kind_A_stats["OpenNumber_success"]
        open_date_success = kind_A_stats["OpenDate_success"]
        stats_output["kind_A_special_stats"] = {
            "target_A_documents": total_A,
            "OpenNumber_success_count": open_num_success,
            "OpenNumber_success_rate_percent": round((open_num_success / total_A * 100), 2),
            "OpenDate_success_count": open_date_success,
            "OpenDate_success_rate_percent": round((open_date_success / total_A * 100), 2)
        }
    else:
        stats_output["kind_A_special_stats"] = {"message": "No Kind 'A' documents processed or found."}

    if desc_fallback_stats.get("total_partial_missing_items", 0) > 0:
        total_partial = desc_fallback_stats["total_partial_missing_items"]
        desc_success = desc_fallback_stats["Description_success_when_partial_missing"]
        stats_output["description_fallback_stats"] = {
            "target_documents_missing_drawings_or_embodiments": total_partial,
            "Description_extraction_success_count": desc_success,
            "Description_extraction_success_rate_percent": round((desc_success / total_partial * 100), 2)
        }
    else:
        stats_output["description_fallback_stats"] = {"message": "No cases of missing drawings/embodiments descriptions, or none processed."}
        
    return stats_output

def save_report_to_json(report_data, file_path, pbar_instance=None):
    """ë³´ê³ ì„œ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
    try:
        with open(file_path, 'w', encoding='utf-8') as f_report:
            json.dump(report_data, f_report, ensure_ascii=False, indent=2)
        message = f"â„¹ï¸ í†µê³„ ë³´ê³ ì„œ ì €ì¥: {os.path.basename(file_path)}" # ë©”ì‹œì§€ ìˆ˜ì •

        if pbar_instance:
            pbar_instance.write(f"      {message}")
        else:
            print(f"   {message}")
    except Exception as e:
        error_message = f"âŒ í†µê³„ ë³´ê³ ì„œ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({os.path.basename(file_path)}): {str(e)}" # ë©”ì‹œì§€ ìˆ˜ì •
        if pbar_instance:
            pbar_instance.write(f"      {error_message}")
        else:
            print(f"   {error_message}")

def save_missing_items_report(missing_items_list, file_path, pbar_instance=None):
    """ëˆ„ë½ í•­ëª© ìƒì„¸ ì •ë³´ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
    try:
        with open(file_path, 'w', encoding='utf-8') as f_report:
            json.dump(missing_items_list, f_report, ensure_ascii=False, indent=2)
        message = f"â„¹ï¸ ëˆ„ë½ í•­ëª© ë³´ê³ ì„œ ì €ì¥: {os.path.basename(file_path)} (ëˆ„ë½ {len(missing_items_list)}ê±´)"
        
        if pbar_instance:
            pbar_instance.write(f"      {message}")
        else:
            print(f"   {message}")
    except Exception as e:
        error_message = f"âŒ ëˆ„ë½ í•­ëª© ë³´ê³ ì„œ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({os.path.basename(file_path)}): {str(e)}"
        if pbar_instance:
            pbar_instance.write(f"      {error_message}")
        else:
            print(f"   {error_message}")

def process_year(year, files, output_folder, max_items_per_file, max_file_size_gb, cpu_count_val):
    """ì—°ë„ë³„ XML íŒŒì¼ ì²˜ë¦¬ ë° JSON ë³€í™˜, ì²­í¬ë³„/ì—°ë„ë³„ ìƒì„¸ ë³´ê³ ì„œ ìƒì„±"""
    if not files:
        print(f"âš ï¸ {year}ë…„ì—ëŠ” ì²˜ë¦¬í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return 0, 0, [], {} 
        
    print(f"\nğŸ“… {year}ë…„ ë°ì´í„° ì²˜ë¦¬ ì¤‘... (ì´ {len(files)}ê°œ íŒŒì¼)")
    
    year_output_dir = os.path.join(output_folder, year)
    os.makedirs(year_output_dir, exist_ok=True)
    
    chunk_count = 0
    current_chunk_data = [] 
    current_size_bytes = 0
    max_size_bytes = max_file_size_gb * 1024 * 1024 * 1024
    
    success_count = 0
    fail_count = 0
    failed_files = []
    
    # ì—°ë„ ì „ì²´ í†µê³„ ë° ëˆ„ë½ ë³´ê³ 
    total_processed_for_stats_year = 0
    overall_field_success_counts_year = defaultdict(int)
    kind_A_stats_year = {"OpenNumber_success": 0, "OpenDate_success": 0, "total_A_items": 0}
    desc_fallback_stats_year = {"Description_success_when_partial_missing": 0, "total_partial_missing_items": 0}
    yearly_total_missing_item_reports = [] 
    
    # í˜„ì¬ ì²­í¬ í†µê³„ ë° ëˆ„ë½ ë³´ê³ 
    current_chunk_items_count = 0
    current_chunk_field_success_counts = defaultdict(int)
    current_chunk_kind_A_stats = {"OpenNumber_success": 0, "OpenDate_success": 0, "total_A_items": 0}
    current_chunk_desc_fallback_stats = {"Description_success_when_partial_missing": 0, "total_partial_missing_items": 0}
    current_chunk_missing_details_list = []

    start_time = time.time()
    try:
        with Pool(processes=cpu_count_val) as pool:
            with tqdm(total=len(files), desc=f"{year}ë…„ ë³€í™˜ ì§„í–‰ë¥ ", unit="íŒŒì¼", 
                     ncols=100, ascii=True, mininterval=0.1) as pbar:
                
                for result in pool.imap_unordered(process_xml_file, files, chunksize=8):
                    file_path, parsed_data, error, extraction_status, missing_details = result
                    
                    if error:
                        fail_count += 1
                        failed_files.append((file_path, error))
                    else:
                        success_count += 1
                        if parsed_data:
                            # --- ì—°ë„ ì „ì²´ í†µê³„ ì—…ë°ì´íŠ¸ ---
                            total_processed_for_stats_year += 1
                            for field, extracted in extraction_status.items():
                                if extracted: overall_field_success_counts_year[field] += 1
                            if parsed_data.get("Kind") == 'A':
                                kind_A_stats_year["total_A_items"] += 1
                                if extraction_status.get("OpenNumber", False): kind_A_stats_year["OpenNumber_success"] += 1
                                if extraction_status.get("OpenDate", False): kind_A_stats_year["OpenDate_success"] += 1
                            brief_drawings_year = bool(parsed_data.get("BriefDescriptionOfDrawings"))
                            embodiments_year = bool(parsed_data.get("DescriptionOfEmbodiments"))
                            if not brief_drawings_year or not embodiments_year:
                                desc_fallback_stats_year["total_partial_missing_items"] += 1
                                if extraction_status.get("Description", False): desc_fallback_stats_year["Description_success_when_partial_missing"] += 1
                            if missing_details:
                                yearly_total_missing_item_reports.append(missing_details)
                                current_chunk_missing_details_list.append(missing_details)

                            # --- í˜„ì¬ ì²­í¬ í†µê³„ ì—…ë°ì´íŠ¸ ---
                            current_chunk_items_count +=1
                            for field, extracted in extraction_status.items():
                                if extracted: current_chunk_field_success_counts[field] += 1
                            if parsed_data.get("Kind") == 'A':
                                current_chunk_kind_A_stats["total_A_items"] += 1
                                if extraction_status.get("OpenNumber", False): current_chunk_kind_A_stats["OpenNumber_success"] += 1
                                if extraction_status.get("OpenDate", False): current_chunk_kind_A_stats["OpenDate_success"] += 1
                            brief_drawings_chunk = bool(parsed_data.get("BriefDescriptionOfDrawings"))
                            embodiments_chunk = bool(parsed_data.get("DescriptionOfEmbodiments"))
                            if not brief_drawings_chunk or not embodiments_chunk:
                                current_chunk_desc_fallback_stats["total_partial_missing_items"] += 1
                                if extraction_status.get("Description", False): current_chunk_desc_fallback_stats["Description_success_when_partial_missing"] += 1
                            
                            item_json = json.dumps(parsed_data, ensure_ascii=False)
                            item_size = len(item_json.encode('utf-8'))

                            if current_chunk_data and (len(current_chunk_data) >= max_items_per_file or current_size_bytes + item_size > max_size_bytes):
                                chunk_count += 1
                                chunk_file_name_base = f"{year}_chunk_{chunk_count}"
                                
                                # ë°ì´í„° ì²­í¬ ì €ì¥
                                chunk_data_file_path = os.path.join(year_output_dir, f"{chunk_file_name_base}.json")
                                save_report_to_json(current_chunk_data, chunk_data_file_path, pbar) # save_report_to_json ì‚¬ìš© (ë‹¨, ë©”ì‹œì§€ ì»¤ìŠ¤í…€ í•„ìš”)
                                # ìœ„ save_report_to_jsonì€ ì¼ë°˜ ë°ì´í„°ìš©ì´ë¯€ë¡œ, ê¸°ì¡´ print ìœ ì§€ ë˜ëŠ” ë³„ë„ í•¨ìˆ˜
                                with open(chunk_data_file_path, 'w', encoding='utf-8') as f_data:
                                     json.dump(current_chunk_data, f_data, ensure_ascii=False, indent=2)
                                chunk_size_mb = os.path.getsize(chunk_data_file_path) / (1024 * 1024)
                                pbar.write(f"\n   âœ… ë°ì´í„° ì²­í¬ ì €ì¥: {os.path.basename(chunk_data_file_path)} (í•­ëª© {len(current_chunk_data)}ê°œ, {chunk_size_mb:.2f}MB)")


                                # ì²­í¬ë³„ í†µê³„ ë³´ê³ ì„œ ì €ì¥
                                chunk_stats_report_data = generate_stats_dict(
                                    current_chunk_items_count,
                                    current_chunk_field_success_counts,
                                    current_chunk_kind_A_stats,
                                    current_chunk_desc_fallback_stats
                                    # current_chunk_missing_details_list # generate_stats_dictì—ì„œ ì œê±°ë¨
                                )
                                chunk_stats_report_file_path = os.path.join(year_output_dir, f"{chunk_file_name_base}_extraction_stats_report.json") # íŒŒì¼ëª… ë³€ê²½
                                save_report_to_json(chunk_stats_report_data, chunk_stats_report_file_path, pbar)
                                
                                # ì²­í¬ë³„ ëˆ„ë½ í•­ëª© ë³´ê³ ì„œ ì €ì¥
                                if current_chunk_missing_details_list:
                                    chunk_missing_items_file_path = os.path.join(year_output_dir, f"{chunk_file_name_base}_missing_items_report.json") # ìƒˆ íŒŒì¼ëª…
                                    save_missing_items_report(current_chunk_missing_details_list, chunk_missing_items_file_path, pbar)
                                
                                # í„°ë¯¸ë„ì—ëŠ” ê°„ëµí•œ ìš”ì•½ ë˜ëŠ” ê¸°ì¡´ í†µê³„ ì¶œë ¥ ìœ ì§€ (ì„ íƒ ì‚¬í•­)
                                # pbar.write(f"      ğŸ“Š ì²­í¬ ë‚´ í•­ëª©ë³„ ì¶”ì¶œ ì„±ê³µë¥  ({current_chunk_items_count}ê°œ ë¬¸ì„œ ê¸°ì¤€): ...") # ê¸°ì¡´ ìƒì„¸ ì¶œë ¥ ëŒ€ì‹  íŒŒì¼ ì €ì¥ ì•Œë¦¼ìœ¼ë¡œ ëŒ€ì²´ ê°€ëŠ¥

                                # í˜„ì¬ ì²­í¬ ë³€ìˆ˜ ì´ˆê¸°í™”
                                current_chunk_data = []
                                current_size_bytes = 0
                                current_chunk_items_count = 0
                                current_chunk_field_success_counts = defaultdict(int)
                                current_chunk_kind_A_stats = {"OpenNumber_success": 0, "OpenDate_success": 0, "total_A_items": 0}
                                current_chunk_desc_fallback_stats = {"Description_success_when_partial_missing": 0, "total_partial_missing_items": 0}
                                current_chunk_missing_details_list = []
                            
                            current_chunk_data.append(parsed_data)
                            current_size_bytes += item_size
                    
                    pbar.update(1)
                    pbar.set_postfix({'ì„±ê³µ': success_count, 'ì‹¤íŒ¨': fail_count})
        
        # ë§ˆì§€ë§‰ ë‚¨ì€ ì²­í¬ ì²˜ë¦¬
        if current_chunk_data: 
            chunk_count += 1
            chunk_file_name_base = f"{year}_chunk_{chunk_count}"
            
            chunk_data_file_path = os.path.join(year_output_dir, f"{chunk_file_name_base}.json")
            # save_report_to_json(current_chunk_data, chunk_data_file_path) # ì¼ë°˜ ë°ì´í„° ì €ì¥
            with open(chunk_data_file_path, 'w', encoding='utf-8') as f_data:
                 json.dump(current_chunk_data, f_data, ensure_ascii=False, indent=2)
            chunk_size_mb = os.path.getsize(chunk_data_file_path) / (1024 * 1024)
            print(f"\n   âœ… ë§ˆì§€ë§‰ ë°ì´í„° ì²­í¬ ì €ì¥: {os.path.basename(chunk_data_file_path)} (í•­ëª© {len(current_chunk_data)}ê°œ, {chunk_size_mb:.2f}MB)")


            chunk_stats_report_data = generate_stats_dict( # ë³€ìˆ˜ëª… ë³€ê²½ chunk_report_data -> chunk_stats_report_data
                current_chunk_items_count,
                current_chunk_field_success_counts,
                current_chunk_kind_A_stats,
                current_chunk_desc_fallback_stats
                # current_chunk_missing_details_list # generate_stats_dictì—ì„œ ì œê±°ë¨
            )
            chunk_stats_report_file_path = os.path.join(year_output_dir, f"{chunk_file_name_base}_extraction_stats_report.json") # íŒŒì¼ëª… ë³€ê²½
            save_report_to_json(chunk_stats_report_data, chunk_stats_report_file_path) # pbar ì—†ìŒ

            # ë§ˆì§€ë§‰ ì²­í¬ì˜ ëˆ„ë½ í•­ëª© ë³´ê³ ì„œ ì €ì¥
            if current_chunk_missing_details_list:
                chunk_missing_items_file_path = os.path.join(year_output_dir, f"{chunk_file_name_base}_missing_items_report.json") # ìƒˆ íŒŒì¼ëª…
                save_missing_items_report(current_chunk_missing_details_list, chunk_missing_items_file_path) # pbar ì—†ìŒ

    except KeyboardInterrupt:
        # ... (ì´í•˜ ë™ì¼)
        print("\n\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        # ì¤‘ë‹¨ ì‹œì—ë„ í˜„ì¬ê¹Œì§€ì˜ ì—°ë„ ì „ì²´ í†µê³„ ë° ëˆ„ë½ ë³´ê³ ëŠ” ë°˜í™˜í•  ìˆ˜ ìˆë„ë¡ í•¨
        year_final_stats_report_data_on_interrupt = generate_stats_dict( # ë³€ìˆ˜ëª… ë³€ê²½
            total_processed_for_stats_year,
            overall_field_success_counts_year,
            kind_A_stats_year,
            desc_fallback_stats_year
            # yearly_total_missing_item_reports # generate_stats_dictì—ì„œ ì œê±°ë¨
        )
        # ì¤‘ë‹¨ ì‹œ ëˆ„ë½ ë³´ê³ ëŠ” ë³„ë„ë¡œ ì²˜ë¦¬í•˜ì§€ ì•Šê±°ë‚˜, í•„ìš”ì‹œ ì €ì¥ ë¡œì§ ì¶”ê°€ ê°€ëŠ¥ (í˜„ì¬ëŠ” í†µê³„ë§Œ ë°˜í™˜)
        return success_count, fail_count, failed_files, {"full_year_stats_report_data": year_final_stats_report_data_on_interrupt, "yearly_missing_items": yearly_total_missing_item_reports}


    except Exception as e:
        print(f"\nâŒ {year}ë…„ ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        print(traceback.format_exc())
        year_final_stats_report_data_on_error = generate_stats_dict( # ë³€ìˆ˜ëª… ë³€ê²½
            total_processed_for_stats_year,
            overall_field_success_counts_year,
            kind_A_stats_year,
            desc_fallback_stats_year
            # yearly_total_missing_item_reports # generate_stats_dictì—ì„œ ì œê±°ë¨
        )
        return success_count, fail_count, failed_files, {"full_year_stats_report_data": year_final_stats_report_data_on_error, "yearly_missing_items": yearly_total_missing_item_reports}

    # --- ì—°ë„ë³„ ìµœì¢… ë³´ê³ ì„œ ìƒì„± ë° ì €ì¥ ---
    year_final_stats_report_data = generate_stats_dict( # ë³€ìˆ˜ëª… ë³€ê²½
        total_processed_for_stats_year,
        overall_field_success_counts_year,
        kind_A_stats_year,
        desc_fallback_stats_year
        # yearly_total_missing_item_reports # generate_stats_dictì—ì„œ ì œê±°ë¨
    )
    year_stats_report_file_path = os.path.join(year_output_dir, f"{year}_extraction_stats_report.json") # íŒŒì¼ëª… ë³€ê²½
    save_report_to_json(year_final_stats_report_data, year_stats_report_file_path)
    
    # ì—°ë„ë³„ ëˆ„ë½ í•­ëª© ë³´ê³ ì„œ ì €ì¥
    if yearly_total_missing_item_reports:
        year_missing_items_file_path = os.path.join(year_output_dir, f"{year}_missing_items_report.json") # ìƒˆ íŒŒì¼ëª…
        save_missing_items_report(yearly_total_missing_item_reports, year_missing_items_file_path)

    # í„°ë¯¸ë„ì—ëŠ” ì—°ë„ë³„ ìš”ì•½ë§Œ ì¶œë ¥ (ëˆ„ë½ ê±´ìˆ˜ ë¼ì¸ ì œê±°)
    print(f"\nğŸ“Š {year}ë…„ ìµœì¢… ì¶”ì¶œ í†µê³„ ìš”ì•½ (ìƒì„¸ ë‚´ìš©ì€ '{os.path.basename(year_stats_report_file_path)}' ë° ê´€ë ¨ ëˆ„ë½ ë³´ê³ ì„œ ì°¸ì¡°):") # ë©”ì‹œì§€ ìˆ˜ì •
    print(f"   - ì´ ì²˜ë¦¬ ë¬¸ì„œ (ìœ íš¨): {total_processed_for_stats_year}")

    return success_count, fail_count, failed_files, {"full_year_stats_report_data": year_final_stats_report_data, "yearly_missing_items": yearly_total_missing_item_reports} # ë°˜í™˜ê°’ì— yearly_missing_items ì¶”ê°€

def batch_convert(input_folder: str, output_folder: str, target_folders: list = None, max_items_per_file: int = 50000, max_file_size_gb: int = 5):
    if not os.path.exists(input_folder):
        print(f"âŒ ì…ë ¥ í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤ â†’ {input_folder}")
        return

    print(f"\nğŸ“‚ ì…ë ¥ í´ë”: {input_folder}")
    print(f"ğŸ“‚ ì¶œë ¥ í´ë”: {output_folder}")
    print(f"ğŸ¯ ëŒ€ìƒ í´ë”: {target_folders if target_folders else 'ì „ì²´'}")
    print(f"ğŸ“Š íŒŒì¼ë‹¹ ìµœëŒ€ í•­ëª© ìˆ˜: {max_items_per_file}ê°œ")
    print(f"ğŸ“Š íŒŒì¼ë‹¹ ìµœëŒ€ í¬ê¸°: {max_file_size_gb}GB")
    
    print("\nğŸ” í´ë” êµ¬ì¡° í™•ì¸ ì¤‘...")
    xml_files_by_year = {}
    folder_stats = {} 

    cpu_count_val = 5 
    print(f"ğŸ’¿ ì‚¬ìš©í•  CPU ì½”ì–´ ìˆ˜: {cpu_count_val}")

    total_success_all_years = 0
    total_fail_all_years = 0
    all_failed_files_reports = [] # ì „ì²´ ì‹¤íŒ¨ ë³´ê³ ì„œë¥¼ ë‹´ì„ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
    
    grand_total_processed_items = 0
    grand_overall_field_success_counts = defaultdict(int)
    grand_kind_A_stats = {"OpenNumber_success": 0, "OpenDate_success": 0, "total_A_items": 0}
    grand_desc_fallback_stats = {"Description_success_when_partial_missing": 0, "total_partial_missing_items": 0}
    grand_missing_item_reports_list = []

    if not target_folders: 
        target_folders = [d for d in os.listdir(input_folder) if os.path.isdir(os.path.join(input_folder, d))]
        print(f"â„¹ï¸ ëŒ€ìƒ í´ë”ê°€ ì§€ì •ë˜ì§€ ì•Šì•„ ì…ë ¥ í´ë” ë‚´ ëª¨ë“  ì—°ë„ í´ë”ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤: {target_folders}")

    for year in target_folders:
        try:
            xml_files_by_year[year] = collect_xmls_for_year(input_folder, year)
            folder_stats[year] = len(xml_files_by_year[year])
            if not os.path.exists(output_folder):
                os.makedirs(output_folder, exist_ok=True)
            
            success, fail, failed_files, year_result_data = process_year(
                year, xml_files_by_year[year], output_folder, max_items_per_file, max_file_size_gb, cpu_count_val
            )
                
            total_success_all_years += success
            total_fail_all_years += fail
            
            if year_result_data and "full_year_stats_report_data" in year_result_data: # í‚¤ ì´ë¦„ ë³€ê²½
                year_stats_report = year_result_data["full_year_stats_report_data"] # ë³€ìˆ˜ëª… ë³€ê²½
                # print(f"DEBUG: Processing year_stats_report for {year}: {year_stats_report}") # ë””ë²„ê¹…ìš©

                current_year_processed_items = year_stats_report.get("summary", {}).get("total_processed_items", 0)
                grand_total_processed_items += current_year_processed_items
                
                # field_extraction_success_rateëŠ” ë¦¬ìŠ¤íŠ¸, ê° ìš”ì†ŒëŠ” dict {"field": name, "success_count": count, ...}
                for field_stat in year_stats_report.get("field_extraction_success_rate", []):
                    field_name = field_stat.get("field")
                    success_cnt = field_stat.get("success_count", 0)
                    if field_name: # í•„ë“œ ì´ë¦„ì´ ìˆì–´ì•¼ í•©ì‚° ê°€ëŠ¥
                         grand_overall_field_success_counts[field_name] += success_cnt
                
                year_kind_A = year_stats_report.get("kind_A_special_stats", {})
                # generate_stats_dictì—ì„œ ë©”ì‹œì§€ê°€ ì•„ë‹Œ ì‹¤ì œ ë°ì´í„°ì¼ ë•Œë§Œ í‚¤ë“¤ì´ ì¡´ì¬í•¨
                if "target_A_documents" in year_kind_A:
                    grand_kind_A_stats["total_A_items"] += year_kind_A.get("target_A_documents", 0)
                    grand_kind_A_stats["OpenNumber_success"] += year_kind_A.get("OpenNumber_success_count", 0)
                    grand_kind_A_stats["OpenDate_success"] += year_kind_A.get("OpenDate_success_count", 0)

                year_desc_fallback = year_stats_report.get("description_fallback_stats", {})
                if "target_documents_missing_drawings_or_embodiments" in year_desc_fallback:
                    grand_desc_fallback_stats["total_partial_missing_items"] += year_desc_fallback.get("target_documents_missing_drawings_or_embodiments", 0)
                    grand_desc_fallback_stats["Description_success_when_partial_missing"] += year_desc_fallback.get("Description_extraction_success_count", 0)
                
            if year_result_data and "yearly_missing_items" in year_result_data: # ëˆ„ë½ ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸ í•©ì‚°
                 grand_missing_item_reports_list.extend(year_result_data.get("yearly_missing_items", []))
            # else:
                # print(f"DEBUG: No valid 'full_year_stats_report_data' or 'yearly_missing_items' for year {year}")

            # process_yearë¡œë¶€í„° ë°›ì€ failed_filesë¥¼ all_failed_files_reportsì— ì¶”ê°€
            if failed_files: # failed_filesê°€ Noneì´ ì•„ë‹ˆê³  ë‚´ìš©ì´ ìˆì„ ë•Œ
                all_failed_files_reports.extend(failed_files)

            print(f"\nâœ… {year}ë…„ ì²˜ë¦¬ ì™„ë£Œ â€“ ì„±ê³µ {success}ê±´ / ì‹¤íŒ¨ {fail}ê±´")
            
        except Exception as e:
            print(f"âŒ {year}ë…„ ì²˜ë¦¬ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            print(traceback.format_exc())

    # print(f"DEBUG: ìµœì¢… í•©ì‚°ëœ grand_total_processed_items: {grand_total_processed_items}") # ë””ë²„ê¹…
    # print(f"DEBUG: ìµœì¢… í•©ì‚°ëœ grand_overall_field_success_counts: {dict(grand_overall_field_success_counts)}") # ë””ë²„ê¹…
    # print(f"DEBUG: ìµœì¢… í•©ì‚°ëœ grand_kind_A_stats: {grand_kind_A_stats}") # ë””ë²„ê¹…
    # print(f"DEBUG: ìµœì¢… í•©ì‚°ëœ grand_desc_fallback_stats: {grand_desc_fallback_stats}") # ë””ë²„ê¹…


    print(f"\nğŸğŸğŸ ì „ì²´ ë³€í™˜ ì‘ì—… ì™„ë£Œ â€“ ì´ ì„±ê³µ {total_success_all_years}ê±´ / ì´ ì‹¤íŒ¨ {total_fail_all_years}ê±´ ğŸğŸğŸ")

    overall_stats_report_data = generate_stats_dict( # ë³€ìˆ˜ëª… ë³€ê²½
        grand_total_processed_items,
        grand_overall_field_success_counts, 
        grand_kind_A_stats, 
        grand_desc_fallback_stats
        # grand_missing_item_reports_list # generate_stats_dictì—ì„œ ì œê±°ë¨
    )
    
    overall_stats_report_file_path = os.path.join(output_folder, "overall_extraction_stats_report.json") # íŒŒì¼ëª… ë³€ê²½
    save_report_to_json(overall_stats_report_data, overall_stats_report_file_path)

    # ì „ì²´ ëˆ„ë½ í•­ëª© ë³´ê³ ì„œ ì €ì¥
    if grand_missing_item_reports_list:
        overall_missing_items_file_path = os.path.join(output_folder, "overall_missing_items_report.json") # ìƒˆ íŒŒì¼ëª…
        save_missing_items_report(grand_missing_item_reports_list, overall_missing_items_file_path)


    # í„°ë¯¸ë„ì—ëŠ” ì „ì²´ ìš”ì•½ë§Œ ê°„ëµíˆ (ëˆ„ë½ ê±´ìˆ˜ ë¼ì¸ ì œê±°)
    print(f"\n\nğŸ“Š ì „ì²´ ê¸°ê°„ ìµœì¢… ì¶”ì¶œ í†µê³„ ìš”ì•½ (ìƒì„¸ ë‚´ìš©ì€ '{os.path.basename(overall_stats_report_file_path)}' ë° '{os.path.basename(overall_missing_items_file_path)}' ì°¸ì¡°):") # ë©”ì‹œì§€ ìˆ˜ì •
    print(f"   - ì´ ì²˜ë¦¬ ë¬¸ì„œ (ìœ íš¨): {grand_total_processed_items}")
    # ì•„ë˜ ë¼ì¸ ì œê±°:
    # if grand_missing_item_reports_list:
    #     print(f"   - ëˆ„ë½ í•­ëª© ë°œìƒ ê±´ìˆ˜ (ì „ì²´): {len(grand_missing_item_reports_list)}")
    # else:
    #     print(f"   - ì „ì²´ ê¸°ê°„ ë™ì•ˆ ëˆ„ë½ëœ í•­ëª©ì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    # ì „ì²´ ì‹¤íŒ¨ ìƒì„¸ ë³´ê³ ì„œ ì €ì¥ ë¡œì§ ì¶”ê°€
    if all_failed_files_reports:
        failures_log_path = os.path.join(output_folder, "overall_failures.log")
        try:
            with open(failures_log_path, 'w', encoding='utf-8') as f_log:
                for file_path, error_message in all_failed_files_reports:
                    f_log.write(f"File: {file_path}\nError: {error_message}\n--------------------\n") # <--- ì—¬ê¸°ì— ë‹«ëŠ” ë”°ì˜´í‘œ ì¶”ê°€
            print(f"\nâš ï¸ ì „ì²´ ì‹¤íŒ¨ ìƒì„¸ ë³´ê³ ì„œ ({len(all_failed_files_reports)}ê±´)ê°€ {failures_log_path} ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"\nâŒ ì „ì²´ ì‹¤íŒ¨ ìƒì„¸ ë³´ê³ ì„œ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    elif total_fail_all_years > 0: # ì „ì²´ ì‹¤íŒ¨ ì¹´ìš´íŠ¸ëŠ” ìˆìœ¼ë‚˜, ìƒì„¸ ë³´ê³  ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆëŠ” ê²½ìš°
        print(f"\nâš ï¸ ì´ {total_fail_all_years}ê±´ì˜ ì‹¤íŒ¨ê°€ ê¸°ë¡ë˜ì—ˆìœ¼ë‚˜, ìƒì„¸ ì‹¤íŒ¨ ë³´ê³  ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤. ì½”ë“œ ì ê²€ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    # else: # ì‹¤íŒ¨ê°€ ì—†ëŠ” ê²½ìš° íŠ¹ë³„í•œ ë©”ì‹œì§€ ì—†ìŒ

########################################################
# 5ï¸âƒ£  CLI ì§„ì…ì  â€“ í•„ìš”í•˜ë©´ main_batch.py ì—ì„œ í˜¸ì¶œ #
########################################################

if __name__ == "__main__":
    import traceback
    import argparse
    
    parser = argparse.ArgumentParser(description="XML to JSON ë³€í™˜ ë„êµ¬")
    parser.add_argument("--single-file", help="ë‹¨ì¼ íŒŒì¼ë§Œ ì²˜ë¦¬")
    parser.add_argument("--output-dir", default="./output", help="ê²°ê³¼ íŒŒì¼ì´ ì €ì¥ë  ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: ./output)")
    parser.add_argument("--max-items", type=int, default=50000, help="íŒŒì¼ë‹¹ ìµœëŒ€ í•­ëª© ìˆ˜ (ê¸°ë³¸ê°’: 50000)")
    parser.add_argument("--max-size", type=int, default=5, help="íŒŒì¼ë‹¹ ìµœëŒ€ í¬ê¸°(GB) (ê¸°ë³¸ê°’: 5)")
    args = parser.parse_args()
    
    if args.single_file:
        # ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬ ëª¨ë“œ
        print(f"ë‹¨ì¼ íŒŒì¼ '{args.single_file}' ì²˜ë¦¬ ì¤‘...")
        file_path, data, error, extraction_status, missing_details = process_xml_file(args.single_file)
        if error:
            print(f"ì˜¤ë¥˜ ë°œìƒ: {error}")
        else:
            # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
            os.makedirs(args.output_dir, exist_ok=True)
            
            # íŒŒì¼ëª…ë§Œ ì¶”ì¶œ
            file_name = os.path.basename(args.single_file)
            base_name = os.path.splitext(file_name)[0]
            
            # ì¶œë ¥ íŒŒì¼ ê²½ë¡œ ìƒì„±
            output_file = os.path.join(args.output_dir, base_name + '.json')
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump([data], f, ensure_ascii=False, indent=2)
            print(f"ë³€í™˜ ì™„ë£Œ: {output_file}")
    else:
        # ì¼ë°˜ ë°°ì¹˜ ì²˜ë¦¬ ëª¨ë“œ
        # ê¸°ì¡´ Windows ê²½ë¡œ (ë‚˜ì¤‘ì— ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì£¼ì„ ì²˜ë¦¬)
        INPUT_DIR = r"C:\Users\kwoor\Python\folder_arange\CN_ë²ˆì—­"
        # OUTPUT_DIR = r"D:\CN_json_result_2021_2"  # ê¸°ì¡´ ê²°ê³¼ì™€ êµ¬ë¶„í•˜ê¸° ìœ„í•´ ë³„ë„ í´ë” ì‚¬ìš©
        OUTPUT_DIR = r"D:\CN_json_result\2021_datatypetest"
        # target_folders = ["2021"]  # 2021 í´ë” ì „ì²´(CN20, CN21 í•˜ìœ„í´ë” í¬í•¨) ì²˜ë¦¬
        target_folders = [str(year) for year in range(2021, 2022)]
        
        # Mac ê²½ë¡œ (2023ë…„ ë°ì´í„° ì²˜ë¦¬ìš©)
        # INPUT_DIR = "/Users/macsh/Documents/Wand-project/cn/cn_ë²ˆì—­"
        # OUTPUT_DIR = "/Users/macsh/Documents/Wand-project/cn/cn_json_results"
        # target_folders = ["2023"]
        
        print(f"\nğŸ¯ ì²˜ë¦¬í•  ì—°ë„ í´ë”: {target_folders}")
        
        print("\nğŸ” ì…ë ¥ í´ë” ì¡´ì¬ ì—¬ë¶€ í™•ì¸...")
        if not os.path.exists(INPUT_DIR):
            print(f"âŒ ì…ë ¥ í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {INPUT_DIR}")
            exit(1)
            
        print("âœ… ì…ë ¥ í´ë” í™•ì¸ ì™„ë£Œ")
        batch_convert(INPUT_DIR, OUTPUT_DIR, target_folders, args.max_items, args.max_size)

# ë¬¸ì„œì—ì„œ description ê°ì²´ ì°¾ê¸°
def find_description_in_document(root):
    """ë¬¸ì„œì—ì„œ description ê°ì²´ë¥¼ ì°¾ì•„ ë°˜í™˜"""
    description = None
    # 1. application-body/description ê²½ë¡œ í™•ì¸
    app_body = safe_dict_get(root, "application-body", {})
    if isinstance(app_body, dict) and "description" in app_body:
        description = app_body["description"]
    # 2. ì§ì ‘ description ê²½ë¡œ í™•ì¸
    elif "description" in root:
        description = root["description"]
    # 3. cn-description ê²½ë¡œ í™•ì¸ (ì¤‘êµ­ íŠ¹í—ˆ ë¬¸ì„œ)
    elif "cn-description" in root:
        description = root["cn-description"]
    # 4. Description ê²½ë¡œ í™•ì¸ (êµ¬ í˜•ì‹ ë¬¸ì„œ)
    elif "Description" in root:
        description = root["Description"]
    
    return description

# ì„¤ëª… ë‹¨ë½ë“¤(paragraphs) ì¶”ì¶œ í•¨ìˆ˜
def extract_description_paragraphs(description):
    """description ê°ì²´ì—ì„œ ë‹¨ë½ë“¤ì„ ì¶”ì¶œ"""
    if not isinstance(description, dict):
        return []
        
    all_paragraphs = []
    # p íƒœê·¸ ë¦¬ìŠ¤íŠ¸ í™•ì¸
    if "p" in description and isinstance(description["p"], list):
        all_paragraphs = description["p"]
    elif "p" in description:
        all_paragraphs = [description["p"]]
    # Paragraphs íƒœê·¸ ë¦¬ìŠ¤íŠ¸ í™•ì¸
    elif "Paragraphs" in description and isinstance(description["Paragraphs"], list):
        all_paragraphs = description["Paragraphs"]
    elif "Paragraphs" in description:
        all_paragraphs = [description["Paragraphs"]]
    
    return all_paragraphs

# ë„ë©´ ì„¤ëª…ê³¼ ì‹¤ì‹œì˜ˆ ì¶”ì¶œí•˜ëŠ” ê³µí†µ í•¨ìˆ˜
def extract_structured_description(description, all_paragraphs):
    """ë„ë©´ ì„¤ëª…ê³¼ ì‹¤ì‹œì˜ˆë¥¼ ì¶”ì¶œí•˜ëŠ” ê³µí†µ í•¨ìˆ˜"""
    drawing_section = None
    embodiment_section = None
    full_description_text = None
    
    # 1. íƒœê·¸ ê¸°ë°˜ ì¶”ì¶œ ì‹œë„
    if isinstance(description, dict):
        # ë„ë©´ ì„¤ëª… ê´€ë ¨ íƒœê·¸ ì°¾ê¸°
        drawings_desc = None
        for key in ["drawings-description", "description-of-drawings", "DrawingsDescription", "drawings"]:
            if key in description:
                drawings_desc = description[key]
                break
        
        # ì‹¤ì‹œì˜ˆ ê´€ë ¨ íƒœê·¸ ì°¾ê¸°
        embodiments = None
        for key in ["detailed-description", "invention-description", "embodiments", "mode-for-invention", "InventionMode"]:
            if key in description:
                embodiments = description[key]
                break
        
        # íƒœê·¸ ê¸°ë°˜ ê²°ê³¼ ì¶”ì¶œ
        if drawings_desc:
            raw_drawing_text = extract_text(drawings_desc)
            drawing_section = scrub_tables(raw_drawing_text)
        
        if embodiments:
            raw_embodiment_text = extract_text(embodiments)
            embodiment_section = scrub_tables(raw_embodiment_text)
    
    # 2. ë§ˆì»¤ ê¸°ë°˜ ì¶”ì¶œ ì‹œë„ (íƒœê·¸ ê¸°ë°˜ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš°)
    if not drawing_section or not embodiment_section:
        if all_paragraphs:
            marker_drawing_section, marker_embodiment_section, _ = extract_description_sections(all_paragraphs)
            
            if not drawing_section and marker_drawing_section:
                drawing_section = marker_drawing_section
            
            if not embodiment_section and marker_embodiment_section:
                embodiment_section = marker_embodiment_section
    
    # 3. ì „ì²´ description í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì„¹ì…˜ êµ¬ë¶„ ì—†ì´)
    if isinstance(description, dict):
        full_description_text = extract_text(description)
    
    return drawing_section, embodiment_section, full_description_text

# íŒŒì‹± ê²°ê³¼ ìƒì„± í•¨ìˆ˜
def create_parsed_result(main_cpc, kind, open_number, open_date, register_number, register_date,
                       pub_number, pub_date, app_number, app_date, applicant_name, inventor_name,
                       agent_name, title, summary, drawing_section, embodiment_section, full_description_text, claims):
    """íŒŒì‹± ê²°ê³¼ë¥¼ í‘œì¤€í™”ëœ ë”•ì…”ë„ˆë¦¬ë¡œ ìƒì„±"""
    return {
        "MainCPC": main_cpc,
        "Kind": kind,
        "OpenNumber": open_number,
        "OpenDate": open_date,
        "RegisterNumber": register_number,
        "RegisterDate": register_date,
        "PublicationNumber": pub_number,
        "PublicationDate": pub_date,
        "ApplicationNumber": app_number,
        "ApplicationDate": app_date,
        "ApplicantName": applicant_name,
        "InventorName": inventor_name,
        "AgentName": agent_name,
        "Title": title,
        "Summary": summary,
        "SummaryOfInvention": None,  # í•­ìƒ Noneìœ¼ë¡œ ì„¤ì •
        "BriefDescriptionOfDrawings": drawing_section,
        "DescriptionOfEmbodiments": embodiment_section,
        "Description": full_description_text if (not bool(drawing_section) or not bool(embodiment_section)) else None,
        "Claims": claims,
    }

def extract_publication_info(root, biblio):
    """ê³µê°œ ì •ë³´ ì¶”ì¶œ í•¨ìˆ˜"""
    pub_refs = safe_get(biblio, "PublicationReference")
    
    # standard í˜•ì‹ ìš°ì„  ì„ íƒ
    pub_info = {}
    if isinstance(pub_refs, list):
        for ref in pub_refs:
            if isinstance(ref, dict) and ref.get("@dataFormat") == "standard":
                pub_info = ref
                break
    elif isinstance(pub_refs, dict):
        pub_info = pub_refs
    
    # DocumentID ê²€ìƒ‰
    pub_doc = {}
    if "@dataFormat" in pub_info and pub_info["@dataFormat"] == "standard":
        pub_doc = safe_get(pub_info, "DocumentID") or {}
    else:
        all_doc_ids = safe_get(pub_info, "DocumentID")
        if isinstance(all_doc_ids, list):
            for doc in all_doc_ids:
                if isinstance(doc, dict) and "@dataFormat" in doc and doc["@dataFormat"] == "standard":
                    pub_doc = doc
                    break
    
    if not isinstance(pub_doc, dict):
        pub_doc = {}
        
    return pub_info, pub_doc

def extract_application_info(biblio):
    """ì¶œì› ì •ë³´ ì¶”ì¶œ í•¨ìˆ˜"""
    application_number = None
    application_date = None
    
    # 1. ApplicationReference ëª©ë¡ì—ì„œ ì¶”ì¶œ
    app_refs = safe_get(biblio, "ApplicationReference")
    if isinstance(app_refs, list):
        for app_ref in app_refs:
            if not isinstance(app_ref, dict):
                continue
            
            if "@dataFormat" in app_ref and app_ref["@dataFormat"] == "standard":
                doc_id = safe_get(app_ref, "DocumentID")
                if isinstance(doc_id, dict):
                    doc_num = safe_dict_get(doc_id, "DocNumber")
                    doc_date = safe_dict_get(doc_id, "Date")
                    
                    if doc_num:
                        application_number = doc_num
                    if doc_date:
                        application_date = doc_date
                    break
    
    # 2. ë‹¨ì¼ ApplicationReferenceì—ì„œ ì¶”ì¶œ
    if not application_number or not application_date:
        app_info = safe_get(biblio, "ApplicationReference") or {}
        if isinstance(app_info, dict):
            if "@dataFormat" in app_info and app_info["@dataFormat"] == "standard":
                app_doc = safe_get(app_info, "DocumentID") or {}
                if isinstance(app_doc, dict):
                    if not application_number:
                        application_number = safe_dict_get(app_doc, "DocNumber")
                    if not application_date:
                        application_date = safe_dict_get(app_doc, "Date")
            else:
                all_doc_ids = safe_get(app_info, "DocumentID")
                if isinstance(all_doc_ids, list):
                    for doc in all_doc_ids:
                        if isinstance(doc, dict) and "@dataFormat" in doc and doc["@dataFormat"] == "standard":
                            if not application_number:
                                application_number = safe_dict_get(doc, "DocNumber")
                            if not application_date:
                                application_date = safe_dict_get(doc, "Date")
                            break
    
    return application_number, application_date

def extract_cpc_info(root, biblio):
    """CPC/IPC ì •ë³´ ì¶”ì¶œ í•¨ìˆ˜"""
    # ë£¨íŠ¸ ë¬¸ì„œì—ì„œ ì§ì ‘ CPC ì¶”ì¶œ ì‹œë„
    main_cpc = safe_dict_get(root, "@docNumber") and extract_text(safe_get(root, "Text"))
    
    # ClassificationIPCRDetailsì—ì„œ ì¶”ì¶œ
    ipcs = safe_get(biblio, "ClassificationIPCRDetails")
    if ipcs:
        ipc_items = safe_get(ipcs, "ClassificationIPCR")
        if isinstance(ipc_items, list) and ipc_items:
            text = safe_get(ipc_items[0], "Text")
            if text:
                main_cpc = clean_ipc_text(extract_text(text))
        elif isinstance(ipc_items, dict):
            text = safe_get(ipc_items, "Text")
            if text:
                main_cpc = clean_ipc_text(extract_text(text))
    
    # ì •ë¦¬ ì²˜ë¦¬
    if isinstance(main_cpc, list) and main_cpc:
        main_cpc = clean_ipc_text(main_cpc[0])
    elif main_cpc:
        main_cpc = clean_ipc_text(main_cpc)
    
    return main_cpc

def extract_applicant_info(biblio):
    """ì¶œì›ì¸ ì •ë³´ ì¶”ì¶œ í•¨ìˆ˜"""
    applicant_details = safe_get(biblio, "ApplicantDetails")
    applicant = safe_get(applicant_details, "Applicant")
    applicant_name = None
    
    if isinstance(applicant, list) and applicant:
        # ì²« ë²ˆì§¸ ì´ë¦„ë§Œ ì²˜ë¦¬
        address_book = safe_get(applicant[0], "AddressBook")
        if address_book:
            if isinstance(address_book, list) and address_book:
                applicant_name = clean_organization_name(extract_text(safe_get(address_book[0], "Name")))
            elif isinstance(address_book, dict):
                applicant_name = clean_organization_name(extract_text(safe_get(address_book, "Name")))
    elif isinstance(applicant, dict):
        # ë‹¨ì¼ ì§€ì›ì ì²˜ë¦¬
        address_book = safe_get(applicant, "AddressBook")
        if address_book:
            if isinstance(address_book, list) and address_book:
                applicant_name = clean_organization_name(extract_text(safe_get(address_book[0], "Name")))
            elif isinstance(address_book, dict):
                applicant_name = clean_organization_name(extract_text(safe_get(address_book, "Name")))
    
    return applicant_name

def extract_inventor_info(biblio):
    """ë°œëª…ì ì •ë³´ ì¶”ì¶œ í•¨ìˆ˜"""
    inventor_block = safe_get(biblio, "InventorDetails")
    inventor_list = safe_get(inventor_block, "Inventor")
    inventor_name = None
    
    if isinstance(inventor_list, list):
        inventor_names = []
        for inv in inventor_list:
            if not isinstance(inv, dict):
                continue
            address_book = safe_get(inv, "AddressBook")
            if address_book:
                if isinstance(address_book, list) and address_book:
                    name = extract_text(safe_get(address_book[0], "Name"))
                elif isinstance(address_book, dict):
                    name = extract_text(safe_get(address_book, "Name"))
                else:
                    name = None
                    
                if name:
                    inventor_names.append(name)
        if inventor_names:
            inventor_name = ", ".join(inventor_names)
    elif isinstance(inventor_list, dict):
        address_book = safe_get(inventor_list, "AddressBook")
        if address_book:
            if isinstance(address_book, list) and address_book:
                inventor_name = extract_text(safe_get(address_book[0], "Name"))
            elif isinstance(address_book, dict):
                inventor_name = extract_text(safe_get(address_book, "Name"))
    
    return inventor_name

def extract_agent_info(biblio):
    """ëŒ€ë¦¬ì¸ ì •ë³´ ì¶”ì¶œ í•¨ìˆ˜"""
    agent_block = safe_get(biblio, "AgentDetails")
    agent_name = None
    
    if isinstance(agent_block, dict):
        agents = safe_get(agent_block, "Agent")
        if isinstance(agents, list) and agents:
            agency_names = []
            for agent in agents:
                if not isinstance(agent, dict):
                    continue
                
                # 1. ì—ì´ì „íŠ¸ ì´ë¦„ê³¼ ì¡°ì§ ì¶”ì¶œ    
                agent_person_name = extract_text(safe_get(agent, "Name"))
                agent_org_name = clean_organization_name(extract_text(safe_get(agent, "OrganizationName")))
                
                # 2. ì£¼ì†Œë¡ì—ì„œ ì´ë¦„ ì¶”ì¶œ ì‹œë„
                address_book = safe_get(agent, "AddressBook")
                if isinstance(address_book, dict):
                    if not agent_person_name:
                        agent_person_name = extract_text(safe_get(address_book, "Name"))
                    if not agent_org_name:
                        agent_org_name = clean_organization_name(extract_text(safe_get(address_book, "OrganizationName")))
                
                # 3. Agency ì„¹ì…˜ ê²€ìƒ‰
                agency = safe_get(agent, "Agency")
                if isinstance(agency, dict):
                    agency_address_book = safe_get(agency, "AddressBook")
                    if isinstance(agency_address_book, dict):
                        if not agent_org_name:
                            agent_org_name = clean_organization_name(extract_text(safe_get(agency_address_book, "OrganizationName")))
                
                # 4. ì¡°í•©í•˜ì—¬ ì—ì´ì „íŠ¸ ì´ë¦„ ìƒì„±
                if agent_person_name and agent_org_name:
                    agency_names.append(f"{agent_person_name} ({agent_org_name})")
                elif agent_person_name:
                    agency_names.append(agent_person_name)
                elif agent_org_name:
                    agency_names.append(agent_org_name)
            
            if agency_names:
                agent_name = "; ".join(agency_names)
        elif isinstance(agents, dict):
            # ë‹¨ì¼ ì—ì´ì „íŠ¸ ì²˜ë¦¬ ë¡œì§
            agent_person_name = extract_text(safe_get(agents, "Name"))
            agent_org_name = clean_organization_name(extract_text(safe_get(agents, "OrganizationName")))
            
            # ì£¼ì†Œë¡ì—ì„œ ì´ë¦„ ì¶”ì¶œ ì‹œë„
            address_book = safe_get(agents, "AddressBook")
            if isinstance(address_book, dict):
                if not agent_person_name:
                    agent_person_name = extract_text(safe_get(address_book, "Name"))
                if not agent_org_name:
                    agent_org_name = clean_organization_name(extract_text(safe_get(address_book, "OrganizationName")))
            
            # Agency ì„¹ì…˜ ê²€ìƒ‰
            agency = safe_get(agents, "Agency")
            if isinstance(agency, dict):
                agency_address_book = safe_get(agency, "AddressBook")
                if isinstance(agency_address_book, dict):
                    if not agent_org_name:
                        agent_org_name = clean_organization_name(extract_text(safe_get(agency_address_book, "OrganizationName")))
            
            # ì¡°í•©í•˜ì—¬ ì—ì´ì „íŠ¸ ì´ë¦„ ìƒì„±
            if agent_person_name and agent_org_name:
                agent_name = f"{agent_person_name} ({agent_org_name})"
            elif agent_person_name:
                agent_name = agent_person_name
            elif agent_org_name:
                agent_name = agent_org_name
    
    return agent_name

def process_fallback_xml(root):
    """ì•Œ ìˆ˜ ì—†ëŠ” êµ¬ì¡°ì˜ XMLì— ëŒ€í•œ fallback íŒŒì‹± ì²˜ë¦¬"""
    # ê¸°ë³¸ê°’ ì´ˆê¸°í™”
    claims = None
    title = None
    summary = None
    drawing_section = None
    embodiment_section = None
    full_description_text = None
    
    # ê¸°ë³¸ íŒŒì‹± ì‹œë„
    if 'application-body' in root and 'description' in root['application-body']:
        description = root['application-body']['description']
        
        # í…ìŠ¤íŠ¸ ì •ë³´ ì¶”ì¶œ
        claims = extract_claims(root)
        title = extract_text(safe_dict_get(root, 'invention-title'))
        abstract = safe_get(root, 'abstract')
        summary = get_abstract_text(abstract)
        
        # ì„¤ëª… ì„¹ì…˜ ì¶”ì¶œ
        all_paragraphs = extract_description_paragraphs(description)
        drawing_section, embodiment_section, full_description_text = extract_structured_description(description, all_paragraphs)
    else:
        return None
    
    # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
    app_number = safe_get(root, '@applicationNumber')
    app_date = safe_get(root, '@applicationDate')
    pub_number = None  # fallbackì—ì„œëŠ” ê³µê°œë²ˆí˜¸ê°€ ì—†ì„ ìˆ˜ ìˆìŒ
    pub_date = None
    kind = safe_get(root, '@kind') or safe_get(root, 'kind')

    # ë²ˆí˜¸ í˜•ì‹ ë³€í™˜
    app_number_clean, pub_number_formatted, pub_date_formatted, open_number, open_date, register_number, register_date = format_numbers(app_number, pub_number, pub_date, kind, app_date)

    # ê²°ê³¼ ë°˜í™˜
    return create_parsed_result(
        None, kind, open_number, open_date, register_number, register_date,
        pub_number_formatted, pub_date_formatted, app_number_clean, app_date,
        None, None, None, title, summary,
        drawing_section, embodiment_section, full_description_text, claims
    )

# def convert_fixed_files_to_json(fixed_files_list, output_dir):  # json ì˜¤ë¥˜ë¡œê·¸ ê¸°ë°˜ìœ¼ë¡œ xml íŒŒì¼ì„ ìˆ˜ì •í•˜ëŠ” ê²ƒì€ ì¶”í›„ ëŒ€í‘œë‹˜ê³¼ ìƒì˜ í›„ ì§„í–‰í•  ì˜ˆì •.
#     """ìˆ˜ì •ëœ XML íŒŒì¼ì„ JSONìœ¼ë¡œ ë³€í™˜"""
#     if not os.path.exists(fixed_files_list):
#         logging.error(f"ìˆ˜ì •ëœ íŒŒì¼ ëª©ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {fixed_files_list}")
#         return False
    
#     if not os.path.exists(output_dir):
#         os.makedirs(output_dir, exist_ok=True)
    
#     # main_batch_home_refactored.pyì—ì„œ í•„ìš”í•œ í•¨ìˆ˜ë“¤ ê°€ì ¸ì˜¤ê¸°
#     from main_batch_home_refactored import process_xml_file
    
#     success_count = 0
#     fail_count = 0
    
#     with open(fixed_files_list, 'r', encoding='utf-8') as f:
#         file_paths = f.read().splitlines()
    
#     total_files = len(file_paths)
#     logging.info(f"ì´ {total_files}ê°œì˜ ìˆ˜ì •ëœ XML íŒŒì¼ì„ JSONìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.")
    
#     results = []
    
#     for file_path in tqdm(file_paths, desc="JSON ë³€í™˜ ì§„í–‰ë¥ "):
#         try:
#             file_path, parsed_data, error, extraction_status, missing_details = process_xml_file(file_path)
            
#             if error:
#                 fail_count += 1
#                 logging.error(f"JSON ë³€í™˜ ì‹¤íŒ¨: {file_path} - {error}")
#             else:
#                 success_count += 1
#                 if parsed_data:
#                     results.append(parsed_data)
#         except Exception as e:
#             fail_count += 1
#             logging.error(f"JSON ë³€í™˜ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {file_path} - {str(e)}")
    
#     if results:
#         output_file = os.path.join(output_dir, "fixed_xml_converted.json")
#         with open(output_file, 'w', encoding='utf-8') as f:
#             json.dump(results, f, ensure_ascii=False, indent=2)
        
#         logging.info(f"âœ… ë³€í™˜ ì™„ë£Œ: {output_file} (ì„±ê³µ: {success_count}, ì‹¤íŒ¨: {fail_count})")
#         return True
#     else:
#         logging.error("ë³€í™˜í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
#         return False